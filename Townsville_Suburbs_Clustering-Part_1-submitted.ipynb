{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Townsville Suburbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction  \n",
    "Townsville is a major regional center in Queensland, Australia. It has a population of over 193,000. It is a major government, commercial and defence center. The Townsville local government area covers 3,736 square kilometres. The area considered in this analysis is about 180 sq klms. Population density in the urban area is just over 1060 persons per sq km.\n",
    "This project will seek to cluster the urban suburbs of Townsville. The features used in the cluster analysis can broadly be considered as Lifestyle factors. They are: Socio-Economic Indexes; Access to Public Transport; Animal Complaints; Crime Data; Access to Council Community Facilities; Access to Community Venues.  \n",
    "The insights gained from this project would be useful to people moving too, or moving within the Townsville District. It can give people an idea of commonalities and differences between suburbs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Sources:   \n",
    "Data was sourced from several online sites, in Excel and Json file formats.  \n",
    "Excel and Json files from the Australian Bureau of Statistics and the Townsville City Council Website.  \n",
    "Json files retrieved in response to API requests to Qld State Government Crime Database and FourSquare database.  \n",
    "Some data preparation was done in the Excel files beforehand.  \n",
    "Geolocation data was obtained using the OpenStreetMap Nominatim API.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Import all packages required\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas_profiling import ProfileReport\n",
    "import json # library to handle JSON files\n",
    "from pandas.io.json import json_normalize # tranform JSON file into a pandas dataframe\n",
    "import requests\n",
    "\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "\n",
    "import geocoder\n",
    "from geopy.geocoders import Nominatim # convert an address into latitude and longitude values\n",
    "import geopy.distance\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.cm as cmx\n",
    "\n",
    "#%matplotlib inline\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "\n",
    "import folium # map rendering library\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import scipy.cluster.hierarchy as shc\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load suburb names. These were cut and paste from council website into Excel. This allowed for editing of required suburbs\n",
    "# This was the most efficient method to get this list\n",
    "\n",
    "df = pd.read_excel('townsville_suburbs.xlsx')\n",
    "df['Suburb'] = df['Suburb'].str.strip() # strip leading and trailing spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get Townsville latitude and longitude\n",
    "\n",
    "address = 'Townsville, Australia'\n",
    "\n",
    "geolocator = Nominatim(user_agent=\"townsville_explorer\")\n",
    "location = geolocator.geocode(address)\n",
    "latitude = location.latitude\n",
    "longitude = location.longitude\n",
    "print('The geograpical coordinate of Townsville are {}, {}.'.format(latitude, longitude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add suburb latitude and longitude\n",
    "\n",
    "# function to return each suburbs location\n",
    "def findlocation(sub):\n",
    "    city = ', Townsville'\n",
    "    address = sub+city\n",
    "    geolocator = Nominatim(user_agent=\"townsville_explorer\")\n",
    "    location = geolocator.geocode(address)\n",
    "    return location \n",
    "\n",
    "for ind in df.index:\n",
    "    locate = findlocation(df.loc[ind][0])\n",
    "    df.loc[[ind],'latitude'] = locate.latitude\n",
    "    df.loc[[ind],'longitude'] = locate.longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add SocioEconomic Data. Excel spreadsheet sourced from the Australian Bureau of Statistics. Prepared in Excel.\n",
    "# Socioeconomic factors include:\n",
    "# Index of Relative Socio-economic Disadvantage (IRSD)\n",
    "# Index of Relative Socio-economic Advantage and Disadvantage (IRSAD)\n",
    "# Index of Economic Resources (IER) \n",
    "# Index of Education and Occupation (IEO)\n",
    "# A higher number is better.\n",
    "\n",
    "se_df = pd.read_excel('state_suburb_socioeconomic_indexs_2016.xlsx', sheet_name='Sheet1',index_col=None,na_values=['NA'],usecols = \"A:E\")\n",
    "se_df['Suburb'] = se_df['Suburb'].str.strip() # strip leading and trailing spaces from suburb names.\n",
    "df_ec = pd.merge(df, se_df, on='Suburb', how='inner')\n",
    "\n",
    "# convert object data to numeric for processing\n",
    "df_ec['rsed'] = pd.to_numeric(df_ec['rsed'])\n",
    "df_ec['rsead'] = pd.to_numeric(df_ec['rsead'])\n",
    "df_ec['ier'] = pd.to_numeric(df_ec['ier'])\n",
    "df_ec['ieo'] = pd.to_numeric(df_ec['ieo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add bus shelters. Excel spreadsheet from City Council website.\n",
    "# Counts the number of bus stops/shelters within a set radius of the suburb center.\n",
    "# More bus stops indicate better access to public transport\n",
    "\n",
    "pt_df = pd.read_excel('Bus_Shelters_Townsville.xlsx', sheet_name='Sheet1',index_col=None,na_values=['NA'],usecols = \"A:G\")\n",
    "\n",
    "for ind in df_ec.index:\n",
    "    count_stop = 0\n",
    "    for ind2 in pt_df.index:\n",
    "        coords_1 = (df_ec.iloc[ind,1],df_ec.iloc[ind,2])\n",
    "        coords_2 = (pt_df.iloc[ind2,3],pt_df.iloc[ind2,4])\n",
    "        #print(coords_1,coords_2)\n",
    "        if (geopy.distance.distance(coords_1, coords_2).m) <= 500:\n",
    "            count_stop = count_stop + 1\n",
    "    df_ec.loc[[ind],'bus_stop'] = count_stop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Animal Complaints. Excel spreadsheet from City Council website.\n",
    "# This tallies animal(dog) complaints delt with by the Council for the financial year 2019-2020\n",
    "# Dataset is based on the 6 primary complaint categories of\n",
    "# noise, wandering, enclosure, aggressive animal, attack and private impound.\n",
    "\n",
    "ac_df = pd.read_excel('animal-complaints.xlsx', sheet_name='Sheet3',index_col=None,na_values=['NA'],usecols = \"A:H\")\n",
    "ac_df['Suburb'] = ac_df['Suburb'].str.strip()\n",
    "ac_df = ac_df.fillna(0)\n",
    "ac_df.head()\n",
    "df_ec_ac = pd.merge(df_ec, ac_df, on='Suburb', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add crime data. Uses API to access State crime database. Response is a json format file. Extracts data from Json file,\n",
    "# then counts incident as either personal crime or property crime.\n",
    "\n",
    "def get_crimes(suburb):\n",
    "    property_crimes=['Arson', 'Other Property Damage', 'Robbery', 'Stock Related Offences', 'Trespassing and Vagrancy', 'Unlawful Entry', 'Unlawful Use of Motor Vehicle', 'Other Theft (excl. Unlawful Entry)']\n",
    "    person_crimes=['Assault', 'Drug Offences', 'Fraud', 'Gaming Racing & Betting Offences', 'Good Order Offences', 'Handling Stolen Goods', 'Homicide (Murder)', 'Liquor (excl. Drunkenness)', 'Miscellaneous Offences', 'Other Homicide', 'Other Offences Against the Person', 'Prostitution Offences', 'Traffic and Related Offences', 'Weapons Act Offences']\n",
    "    prop_cc = 0\n",
    "    pers_cc = 0\n",
    "    url = 'https://a5c7zwf7e5.execute-api.ap-southeast-2.amazonaws.com/dev/offences?locationType=SUBURB&startDate=07-01-2019&locationName={}&endDate=06-30-2020&format=JSON'.format(suburb)\n",
    "    results = requests.get(url).json()\n",
    "    crimes = []\n",
    "    for ind in range(len(results)):\n",
    "        crimes.append(results[ind]['Type'])\n",
    "        \n",
    "    for ind in range(len(crimes)):\n",
    "        if any(crimes[ind] in x for x in property_crimes):\n",
    "            prop_cc = prop_cc +1\n",
    "        elif any(crimes[ind] in x for x in person_crimes):\n",
    "            pers_cc = pers_cc + 1\n",
    "    return(prop_cc,pers_cc)\n",
    "\n",
    "for ind in range(len(df_ec_ac)):\n",
    "    crime_counts = get_crimes(df_ec_ac.iloc[ind,0])\n",
    "    df_ec_ac.loc[[ind],'prop_cc'] = crime_counts[0]\n",
    "    df_ec_ac.loc[[ind],'pers_cc'] = crime_counts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add community facilities data. Counts council facilities (community halls, pools, library etc) within set distance from center of suburb\n",
    "\n",
    "with open('TCC_Community_Facilities.json') as json_file:\n",
    "       data = json.load(json_file)\n",
    "data2 = data['features']\n",
    "\n",
    "cf_df = pd.DataFrame(columns=['category', 'fac_lat', 'fac_long'])\n",
    "for ind in range(len(data2)):\n",
    "    row = [data['features'][ind]['properties']['category'], data['features'][ind]['geometry']['coordinates'][1], data['features'][ind]['geometry']['coordinates'][0]]\n",
    "    cf_df.loc[len(cf_df)] = row\n",
    "\n",
    "for ind in df_ec_ac.index:\n",
    "    count_cf = 0\n",
    "    for ind2 in cf_df.index:\n",
    "        coords_1 = (df_ec_ac.iloc[ind,1],df_ec_ac.iloc[ind,2])\n",
    "        coords_2 = (cf_df.iloc[ind2,1],cf_df.iloc[ind2,2])\n",
    "        #print(coords_1,coords_2)\n",
    "        if (geopy.distance.distance(coords_1, coords_2).m) <= 1000: #council facilities within 1000m\n",
    "            count_cf = count_cf + 1\n",
    "    df_ec_ac.loc[[ind],'council_fac'] = count_cf   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next section uses FourSquare API to get venue details for each suburb. It counts the number of venues in different categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIENT_ID = '************************' # your Foursquare ID\n",
    "CLIENT_SECRET = '******************' # your Foursquare Secret\n",
    "VERSION = '20180605' # Foursquare API version\n",
    "\n",
    "print('Your credentails:')\n",
    "print('CLIENT_ID: ' + CLIENT_ID)\n",
    "print('CLIENT_SECRET:' + CLIENT_SECRET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Venues are summarized into categories of interest.\n",
    "# Venues were within 1000m of the suburb center. A limit of 500 venues was retrieved per suburb. This was never reached.\n",
    "# While the FourSquare database is useful, it is not well supported in remote, non-USA locations.\n",
    "\n",
    "# Venue categories of interest\n",
    "cafe = ['CafÃ©','Coffee Shop', 'Sandwich Place','Pizza Place','Juice Bar','Ice Cream Shop','Fried Chicken Joint','Burger Joint']\n",
    "restaurant = ['Restaurant', 'Steakhouse']\n",
    "supermarket = [ 'Supermarket', 'Market','Grocery Store']\n",
    "pub = [ 'Pub','Liquor Store','Brewery']\n",
    "entertainment = [ 'Theater', 'Multiplex']\n",
    "recreation = [ 'Pool','Historic Site','Football Stadium','Bowling Alley','Basketball Stadium','Art Museum','Aquarium','Beach']\n",
    "\n",
    "# process each suburb for venues\n",
    "def process_suburb(suburb_result):\n",
    "    suburb_venues = []\n",
    "    for i in range(len(results_sub)):\n",
    "        venue_found = results_sub[i]['venue']['categories'][0]['name']\n",
    "        suburb_venues.append(venue_found)\n",
    "    suburb_venues_new = [('Restaurant') if \"Restaurant\" in item else item for item in suburb_venues]\n",
    "    cafe_count = sum(el in cafe for el in suburb_venues_new)\n",
    "    restaurant_count = sum(el in restaurant for el in suburb_venues_new)\n",
    "    supermarket_count = sum(el in supermarket for el in suburb_venues_new)\n",
    "    pub_count = sum(el in pub for el in suburb_venues_new)\n",
    "    entertainment_count = sum(el in entertainment for el in suburb_venues_new)\n",
    "    recreation_count = sum(el in recreation for el in suburb_venues_new)\n",
    "    \n",
    "    return(cafe_count, restaurant_count, supermarket_count, pub_count, entertainment_count, recreation_count)\n",
    "\n",
    "radius = 1000\n",
    "LIMIT = 500\n",
    "\n",
    "for ind in df_ec_ac.index:\n",
    "    lat_test = df_ec_ac.iloc[ind,1]\n",
    "    long_test = df_ec_ac.iloc[ind,2]\n",
    "    url = 'https://api.foursquare.com/v2/venues/explore?client_id={}&client_secret={}&ll={},{}&v={}&radius={}&limit={}'.format(CLIENT_ID, CLIENT_SECRET, lat_test, long_test, VERSION, radius, LIMIT)\n",
    "    results = requests.get(url).json()\n",
    "    results_sub = results['response']['groups'][0]['items']\n",
    "    venue_counts = process_suburb(results_sub)\n",
    "    df_ec_ac.loc[[ind],'cafe_count'] = venue_counts[0]\n",
    "    df_ec_ac.loc[[ind],'restaurant_count'] = venue_counts[1]\n",
    "    df_ec_ac.loc[[ind],'supermarket_count'] = venue_counts[2]\n",
    "    df_ec_ac.loc[[ind],'pub_count'] = venue_counts[3]    \n",
    "    df_ec_ac.loc[[ind],'entertainment_count'] = venue_counts[4]\n",
    "    df_ec_ac.loc[[ind],'recreation_count'] = venue_counts[5]    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ec_ac.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ec_ac.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_Names = list(df_ec_ac.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ec_ac[column_Names[3:15]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ec_ac[column_Names[15:24]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Animal complaints are worse in:' , '\\n\\r', df_ec_ac.nlargest(3,'Animal Complaints Grand Total')['Suburb'])\n",
    "ax = df_ec_ac.plot.bar(x='Suburb', y=['Animal Complaints Grand Total'], rot=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Property Crime is worse in:' , '\\n\\r', df_ec_ac.nlargest(3,'prop_cc')['Suburb'])\n",
    "print('Personal Crime is worse in:' , '\\n\\r', df_ec_ac.nlargest(3,'pers_cc')['Suburb'])\n",
    "ax = df_ec_ac.plot.bar(x='Suburb', y=['prop_cc','pers_cc'], rot=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The suburbs best serviced with Public Transport are:' , '\\n\\r', df_ec_ac.nlargest(3,'bus_stop')['Suburb'])\n",
    "ax = df_ec_ac.plot.bar(x='Suburb', y=['bus_stop'], rot=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ec_ac['Venue Total'] = df_ec_ac.iloc[:, -6:].sum(axis=1)\n",
    "print('The suburbs best serviced with most Venues are:' , '\\n\\r', df_ec_ac.nlargest(3,'Venue Total')['Suburb'])\n",
    "ax = df_ec_ac.plot.bar(x='Suburb', y=['Venue Total'], rot=90)\n",
    "del df_ec_ac['Venue Total']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "corr = df_ec_ac.corr()\n",
    "corr.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a strong correlation between the number of restaurants (type) venues and the number of cafe (type) venues.\n",
    "There is a moderate correlation between the number of Personal Crime incidents and the number of Pub (type) venues.\n",
    "There is a moderate correlation between the number of Property Crimes incidents and the number of Animal Noise Complaints. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare data for KMeans\n",
    "\n",
    "X = df_ec_ac.values[:,3:]\n",
    "X = np.nan_to_num(X)\n",
    "cluster_dataset = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function returns the WSS score (squared erros for all points)-used to identify elbow point; \n",
    "# the Inertia score (spread of points within a cluster)-used to identify elbow point;\n",
    "# the Silhouette score (how far between clusters)-greater value is better clustering;\n",
    "# for values of k between 1 to kmax\n",
    "\n",
    "def calculate_Errors(points, kmax):\n",
    "    init={}\n",
    "    sse = []\n",
    "    sil = []\n",
    "    for k in range(1, kmax+1):\n",
    "        kmeans = KMeans(n_clusters = k).fit(points)\n",
    "        if k < 2:\n",
    "            sil.append(0)\n",
    "        else:\n",
    "            labels = kmeans.labels_\n",
    "            sil.append(silhouette_score(points, labels, metric = 'euclidean'))\n",
    "        init[k] = kmeans.inertia_ # Inertia: Sum of distances of samples to their closest cluster center\n",
    "        centroids = kmeans.cluster_centers_\n",
    "        pred_clusters = kmeans.predict(points)\n",
    "        curr_sse = 0\n",
    "\n",
    "        # calculate square of Euclidean distance of each point from its cluster center and add to current WSS\n",
    "        for i in range(len(points)):\n",
    "            curr_center = centroids[pred_clusters[i]]\n",
    "            curr_sse += (points[i, 0] - curr_center[0]) ** 2 + (points[i, 1] - curr_center[1]) ** 2\n",
    "        sse.append(curr_sse)\n",
    "\n",
    "    return (sse,init,sil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to plot errors and Silhouette score\n",
    "\n",
    "def plot_Errors(sse_errors2,sil_error2,init_errors):\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    fig, ax1 = plt.subplots()\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "\n",
    "    ax1.set_xlabel('K value')\n",
    "    ax1.set_ylabel('SSE and Inertia', color='b')\n",
    "    ax2.set_ylabel('Silhoutte index', color='brown')\n",
    "\n",
    "    ax1.plot(range(1, kmax+1), sse_errors2, color='red', linestyle='dashed', marker='o',\n",
    "             markerfacecolor='blue', markersize=10, label=\"sse errors: identify elbow point\")\n",
    "\n",
    "    ax1.plot(range(1, kmax+1), list(init_errors.values()), color='green', linestyle='dashed', marker='o',\n",
    "             markerfacecolor='yellow', markersize=10, label=\"inertia measure: identify elbow point\")\n",
    "\n",
    "    ax2.plot(range(1, kmax+1), sil_error2, color='brown', linestyle='dashed', marker='o',\n",
    "             markerfacecolor='violet', markersize=10, label=\"silhouette measure: greater is better\")\n",
    "\n",
    "\n",
    "    plt.title('Error Measures and K Values')\n",
    "    plt.xticks(range(1,kmax+1))\n",
    "\n",
    "    h1, l1 = ax1.get_legend_handles_labels()\n",
    "    h2, l2 = ax2.get_legend_handles_labels()\n",
    "    ax1.legend(h1+h2, l1+l2, loc=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full dataset errors and Silhouette\n",
    "\n",
    "kmax = 20\n",
    "sserrors_all = calculate_Errors(cluster_dataset,kmax)\n",
    "\n",
    "#extract metrics from returned tuple\n",
    "raw_sse_errors = sserrors_all[0]\n",
    "raw_init_errors = sserrors_all[1]\n",
    "raw_sil_errors = sserrors_all[2]\n",
    "\n",
    "#scale metric so they appear on same plot\n",
    "raw_sse_errors2 = [i * 10 for i in raw_sse_errors]\n",
    "raw_sil_error2 = [i * 1 for i in raw_sil_errors]\n",
    "\n",
    "plot_Errors(raw_sse_errors2,raw_sil_error2,raw_init_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA errors and Silhouette\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "pca_data = StandardScaler().fit_transform(X)\n",
    "principalComponents = pca.fit_transform(pca_data)\n",
    "principalDf = pd.DataFrame(data = principalComponents\n",
    "             , columns = ['principal component 1', 'principal component 2', 'principal component 3'])\n",
    "\n",
    "kmax = 20\n",
    "sserrors_all = calculate_Errors(principalComponents,kmax)\n",
    "\n",
    "#extract metrics from returned tuple\n",
    "pca_sse_errors = sserrors_all[0]\n",
    "pca_init_errors = sserrors_all[1]\n",
    "pca_sil_errors = sserrors_all[2]\n",
    "\n",
    "#scale metric so they appear on same plot\n",
    "pca_sse_errors2 = [i * 10 for i in pca_sse_errors]\n",
    "pca_sil_error2 = [i * 1 for i in pca_sil_errors]\n",
    "\n",
    "plot_Errors(pca_sse_errors2,pca_sil_error2,pca_init_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot dendrogram to identify number of clusters to use\n",
    "\n",
    "D = df_ec_ac.values[:,3:]\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.title(\"Suburbs Dendograms\")\n",
    "dend = shc.dendrogram(shc.linkage(D, method='ward'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selection of the best number of Clusters to use  \n",
    "This is a very small dataset for KMeans modelling. I could not find any definitive answer regarding using KMeans on such a small dataset. The lack of a clearly identifiable best k value in the Error plots and the Silhouette Score most likely is due to the small data set. I did try reducing the number of features manually, by summing some features and removing some strongly correlated features, but this did not improve the result. A Principal Components Analysis was done to obtain a much smaller feature set. This gave a better case count to feature set ratio. The error plots and Silhouette Score for the PCA dataset did help to select a k value. Dendrogram plot helped justify the use of 3 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 3\n",
    "\n",
    "k_means = KMeans(init=\"k-means++\", n_clusters=num_clusters, n_init=12)\n",
    "k_means.fit(cluster_dataset)\n",
    "labels = k_means.labels_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize KMeans result for PCA dataset using the Kmeans derived labels (clusters)\n",
    "\n",
    "# Add Labels to PCA result dataframe\n",
    "principalDf[\"Labels\"] = labels\n",
    "\n",
    "def scatter3d(x,y,z, cs, colorsMap='jet'):\n",
    "    cm = plt.get_cmap(colorsMap)\n",
    "    cNorm = matplotlib.colors.Normalize(vmin=min(cs), vmax=max(cs))\n",
    "    scalarMap = cmx.ScalarMappable(norm=cNorm, cmap=cm)\n",
    "    fig = plt.figure()\n",
    "    #ax = Axes3D(fig)\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(x, y, z, c=scalarMap.to_rgba(cs))\n",
    "    scalarMap.set_array(cs)\n",
    "    fig.colorbar(scalarMap)\n",
    "    ax.set_xlabel('principal component 1', fontsize = 10)\n",
    "    ax.set_ylabel('principal component 2', fontsize = 10)\n",
    "    ax.set_zlabel('principal component 3', fontsize = 10)\n",
    "    plt.show()\n",
    "    \n",
    "scatter3d(principalDf['principal component 1'],principalDf['principal component 2'],principalDf['principal component 3'],principalDf['Labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though poorly defined, it is possible to identify the 3 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add cluster labels to main dataset\n",
    "\n",
    "df_ec_ac[\"Labels\"] = labels\n",
    "df_ec_ac.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create map of Townsville using latitude and longitude values\n",
    "\n",
    "f = folium.Figure(width=800, height=500)\n",
    "\n",
    "map_townsville = folium.Map(location=[latitude, longitude], tiles=\"openstreetmap\", zoom_start=11).add_to(f)\n",
    "\n",
    "# set color scheme for the clusters\n",
    "x = np.arange(num_clusters)\n",
    "ys = [i + x + (i*x)**2 for i in range(num_clusters)]\n",
    "colors_array = cm.rainbow(np.linspace(0, 1, len(ys)))\n",
    "rainbow = [colors.rgb2hex(i) for i in colors_array]\n",
    "\n",
    "\n",
    "# add markers to map\n",
    "markers_colors = []\n",
    "for lat, lng, suburb, cluster in zip(df_ec_ac['latitude'], df_ec_ac['longitude'], df_ec_ac['Suburb'], df_ec_ac['Labels']):\n",
    "    #label = folium.Popup(str(poi) + ' Cluster ' + str(cluster), parse_html=True)\n",
    "    label = '{}'.format(suburb)\n",
    "    label = folium.Popup(label, parse_html=True)\n",
    "    folium.CircleMarker(\n",
    "        [lat, lng],\n",
    "        radius=5,\n",
    "        popup=label,\n",
    "        color=rainbow[cluster-1],\n",
    "        fill=True,\n",
    "        fill_color=rainbow[cluster-1],\n",
    "        fill_opacity=0.7,\n",
    "        parse_html=False).add_to(map_townsville)  \n",
    "    \n",
    "map_townsville"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print suburb clusters\n",
    "\n",
    "for k in range(0, num_clusters):\n",
    "    print('Suburbs in Group ',k,'\\n\\r',df_ec_ac.loc[df_ec_ac['Labels'] == k].Suburb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive Statistics for the Different Suburbs ( the Label is the Cluster Number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ec_ac[[\"rsed\",'rsead','ier','ieo','Labels']].groupby(\"Labels\").agg({'rsed': ['min', 'max', 'median','mean'],\n",
    "                                                                 'rsead': ['min', 'max', 'median', 'mean'],\n",
    "                                                                 'ier': ['min', 'max', 'median', 'mean'],\n",
    "                                                                 'ieo': ['min', 'max', 'median', 'mean']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ec_ac[[\"prop_cc\", \"pers_cc\",'Animal Complaints Grand Total','Labels']].groupby(\"Labels\").agg({'prop_cc': ['min', 'max', 'median','mean'],\n",
    "                                                                 'pers_cc': ['min', 'max', 'median', 'mean'],\n",
    "                                                                 'Animal Complaints Grand Total': ['min', 'max', 'median', 'mean'],})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ec_ac[[\"cafe_count\", \"restaurant_count\",'Labels']].groupby(\"Labels\").agg({'cafe_count': ['min', 'max', 'median','mean'],\n",
    "                                                                 'restaurant_count': ['min', 'max', 'median', 'mean']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ec_ac[[\"supermarket_count\",'pub_count','entertainment_count','recreation_count','Labels']].groupby(\"Labels\").agg({'supermarket_count': ['min', 'max', 'median','mean'],\n",
    "                                                                 'pub_count': ['min', 'max', 'median', 'mean'],\n",
    "                                                                 'entertainment_count': ['min', 'max', 'median', 'mean'],\n",
    "                                                                 'recreation_count': ['min', 'max', 'median', 'mean']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boxplots for each feature, for each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for ind in range(len(column_Names)):\n",
    "    if (ind > 2):\n",
    "        sns.boxplot( x=df_ec_ac[\"Labels\"], y=df_ec_ac[column_Names[ind]], width=0.3);\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
